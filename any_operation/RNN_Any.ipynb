{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Any Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import random\n",
    "import time\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "\n",
    "import arithmetic_datasets as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 20000\n",
    "rstart = 0\n",
    "rend = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = ad.gen_data(num_examples, rstart, rend)\n",
    "x_test, y_test = ad.gen_data(50, rstart, rend)\n",
    "rdx = np.array(x_train).flatten()\n",
    "rdy = np.array(y_train).flatten()\n",
    "\n",
    "reverse_dictionary = np.unique(np.append(rdx, rdy))\n",
    "\n",
    "dictionary = {}\n",
    "for x in range(len(reverse_dictionary)):\n",
    "    dictionary[reverse_dictionary[x]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 1)\n",
      "(50, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(num_examples, 3, 1)\n",
    "x_test = x_test.reshape(50, 3, 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0\n",
      "(20000, 3, 1)\n",
      "(50, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[dictionary[num[0]] for num in example] for example in x_train])\n",
    "#x_test = np.array([[dictionary[num[0]] for num in example] for example in x_test])\n",
    "print(np.amax(y_train))\n",
    "\n",
    "y_train = np.array([dictionary[example] for example in y_train])\n",
    "#y_test = np.array([[dictionary[num[0]] for num in example] for example in y_test])\n",
    "\n",
    "x_train = x_train.reshape(num_examples, 3, 1)\n",
    "x_test = x_test.reshape(50, 3, 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 512\n",
    "epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3, 64)             6336      \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 512)               3280896   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 99)                50787     \n",
      "=================================================================\n",
      "Total params: 3,338,019\n",
      "Trainable params: 3,338,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(len(dictionary), 64, input_length=3))\n",
    "# model.add(layers.Dense(64, input_shape=(3,10)))\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "# model.add(layers.LSTM(128))\n",
    "new_shape = (3, 1)\n",
    "# model.add(layers.Dense(64, input_shape=new_shape))\n",
    "\n",
    "\n",
    "rnn_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.LSTMCell(n_hidden),tf.keras.layers.LSTMCell(n_hidden)])\n",
    "# model.add(layers.RNN(rnn_cell, input_length=n_input))\n",
    "model.add(layers.RNN(rnn_cell, input_shape=new_shape))\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(len(dictionary), activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              # List of metrics to monitor\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 4.4848 - sparse_categorical_accuracy: 0.1306\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 4.4439 - sparse_categorical_accuracy: 0.1689\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 4.4191 - sparse_categorical_accuracy: 0.1951\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 4.4092 - sparse_categorical_accuracy: 0.2036\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 23s 116ms/step - loss: 4.3950 - sparse_categorical_accuracy: 0.2182\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 23s 116ms/step - loss: 4.3854 - sparse_categorical_accuracy: 0.2273\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 4.3644 - sparse_categorical_accuracy: 0.2482\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 23s 115ms/step - loss: 4.3438 - sparse_categorical_accuracy: 0.2688\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 24s 118ms/step - loss: 4.3259 - sparse_categorical_accuracy: 0.2866\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 24s 120ms/step - loss: 4.3011 - sparse_categorical_accuracy: 0.3111\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 26s 132ms/step - loss: 4.2622 - sparse_categorical_accuracy: 0.3503\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 24s 121ms/step - loss: 4.2282 - sparse_categorical_accuracy: 0.3843\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 24s 121ms/step - loss: 4.1690 - sparse_categorical_accuracy: 0.4441\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 24s 121ms/step - loss: 4.1190 - sparse_categorical_accuracy: 0.4938\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 26s 129ms/step - loss: 4.0539 - sparse_categorical_accuracy: 0.5594\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 31s 154ms/step - loss: 3.9805 - sparse_categorical_accuracy: 0.6322\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 28s 138ms/step - loss: 3.8828 - sparse_categorical_accuracy: 0.73135s - loss: 3.8931\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 31s 155ms/step - loss: 3.7894 - sparse_categorical_accuracy: 0.82383s - loss: 3.7942 - sparse_cate\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 27s 133ms/step - loss: 3.7018 - sparse_categorical_accuracy: 0.9115\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 26s 131ms/step - loss: 3.6639 - sparse_categorical_accuracy: 0.94915s - loss:\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 26s 129ms/step - loss: 3.6406 - sparse_categorical_accuracy: 0.9720\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 27s 133ms/step - loss: 3.6278 - sparse_categorical_accuracy: 0.98463s - loss: 3.6284 - spar\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 24s 120ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 24s 120ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 26s 128ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 24s 118ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 24s 120ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 25s 126ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 25s 123ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 26s 129ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 3.6254 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 24s 121ms/step - loss: 3.6247 - sparse_categorical_accuracy: 0.9876\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 23s 117ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 23s 115ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 23s 116ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 24s 118ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 26s 132ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 25s 124ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 24s 119ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 24s 119ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 24s 119ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 24s 121ms/step - loss: 3.6234 - sparse_categorical_accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs= epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
