{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create object to automated model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arithmetic_datasets as ad\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Directory Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP\n",
    "cb_filepath = \"checkpoints\"\n",
    "if not os.path.exists(cb_filepath):\n",
    "    os.makedirs(cb_filepath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelCallback(Callback):\n",
    "    def __init__(self, filepath,  n=100):\n",
    "        super(SaveModelCallback, self).__init__()\n",
    "        self.num_epochs = 100\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        super(SaveModelCallback, self).on_epoch_end(epoch, logs)\n",
    "        if epoch % 100 == 0:\n",
    "            self.model.save(f\"{self.filepath}/{self.model.name}/{epoch}.model\")\n",
    "            print(f\"model saved: {self.filepath}/{self.model.name}/{epoch}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smc = SaveModelCallback(cb_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelComparator\n",
    "\n",
    "```\n",
    "mc = ModelComparator()\n",
    "mc.add_model(name, model)\n",
    "mc.add_training_set(name, data)\n",
    "\n",
    "mc.train_models(epochs, batches, **kwargs)\n",
    "\n",
    "mc.compare_min(metric)\n",
    "mc.compare_max(metric)\n",
    "\n",
    "mc.history_for(model_name, training_set_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ModelComparator:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.training_sets = {}\n",
    "        self.testing_sets = {}\n",
    "        self.histories = {}\n",
    "        self.smc_params = { \"filepath\" : \"checkpoints\", \"n\" : 100 }\n",
    "    \n",
    "    def add_model(self, name, model):\n",
    "        self.models[name] = model\n",
    "    \n",
    "    def add_training_set(self, name, data):\n",
    "        self.training_sets[name] = data\n",
    "        \n",
    "    def add_testing_set(self, name, data):\n",
    "        self.testing_sets[name] = data\n",
    "        \n",
    "    def set_smc_params(self, filepath, n):\n",
    "        self.smc_params[\"filepath\"] = filepath\n",
    "        self.smc_params[\"n\"] = n\n",
    "        \n",
    "    def train_models(self, **kwargs):\n",
    "        for model_name in self.models:\n",
    "            for tset_name in self.training_sets:\n",
    "                print(f\"MODEL {model_name} TRAINING ON DATASET {tset_name}\")\n",
    "                \n",
    "                start_dir = self.smc_params[\"filepath\"]\n",
    "                fpath = f\"{start_dir}/{tset_name}/{model_name}\"\n",
    "                nn = self.smc_params[\"n\"]\n",
    "                \n",
    "                if not os.path.exists(start_dir):\n",
    "                    os.makedirs(start_dir)\n",
    "                if not os.path.exists(f\"{start_dir}/{tset_name}\"):\n",
    "                    os.makedirs(f\"{start_dir}/{tset_name}\")\n",
    "                if not os.path.exists(fpath):\n",
    "                    os.makedirs(fpath)\n",
    "                \n",
    "                model = None\n",
    "                checkpoints = [cb_filepath + '/' + model_name + \"/\" + name\n",
    "               for name in os.listdir(fpath)]\n",
    "                if checkpoints:\n",
    "                    latest_cp = max(checkpoints, key=os.path.getctime )\n",
    "                    print('Restoring from', latest_cp)\n",
    "                    model = load_model(latest_cp)\n",
    "                \n",
    "                if model == None:\n",
    "                    model = self.models[model_name]()\n",
    "                \n",
    "                smc = SaveModelComparator(filepath=fpath, n=nn)\n",
    "                if \"callbacks\" in kwargs:\n",
    "                    kwargs[\"callbacks\"].append()\n",
    "                \n",
    "                history = model.fit(\n",
    "                    self.training_sets[tset_name][0], self.training_sets[tset_name][1],\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                self.histories[(model_name, tset_name)] = history\n",
    "    \n",
    "    def compare_min(self, metric):\n",
    "        result = {}\n",
    "        minimum = None\n",
    "        min_name = None\n",
    "        for hkey in self.histories:\n",
    "            history = self.histories[hkey].history\n",
    "            result[hkey] = np.amin(history[metric])\n",
    "            if minimum == None or minimum >= result[hkey]:\n",
    "                minimum = result[hkey]\n",
    "                min_name = hkey\n",
    "        return result, {\"name\" : min_name, \"value\":minimum}\n",
    "            \n",
    "    \n",
    "    def compare_max(self, metric):\n",
    "        result = {}\n",
    "        maximum = None\n",
    "        max_name = None\n",
    "        for hkey in self.histories:\n",
    "            history = self.histories[hkey].history\n",
    "            result[hkey] = np.amax(history[metric])\n",
    "            if maximum == None or maximum >= result[hkey]:\n",
    "                maximum = result[hkey]\n",
    "                max_name = hkey\n",
    "        return result, {\"name\" : max_name, \"value\": maximum}\n",
    "    \n",
    "    def history_for(self, model_name, training_set_name):\n",
    "        return self.histories[(model_name, training_set_name)]\n",
    "    \n",
    "    def test_models(self, use_training_sets=False, acc_range=0.5):\n",
    "        if use_training_sets:\n",
    "            testing_sets = self.training_sets\n",
    "        else:\n",
    "            testing_sets = self.testing_sets\n",
    "        \n",
    "        for model_name in self.models:\n",
    "            for tset_name in testing_sets:\n",
    "                print(f\"MODEL {model_name} TESTING ON DATASET {tset_name}\")\n",
    "                model = self.models[model_name]()\n",
    "                \n",
    "                x_test = testing_sets[tset_name][0]\n",
    "                y_test = testing_sets[tset_name][1]\n",
    "                \n",
    "                pred = model.predict(x_test)\n",
    "                \n",
    "                test_total = 0\n",
    "                acc_count = 0\n",
    "                for a in range(len(x_test)):\n",
    "                    x = x_test[a]\n",
    "                    y = y_test[a]\n",
    "                    yp = pred[a][0]\n",
    "                    print(f\"\\tinput={x[0]} expected_output={[y]} prediction={yp} \\n\\t\\tdifference={yp - y} \\n\\t\\t%-different from actual={abs(yp - y)/y}\")\n",
    "\n",
    "                    if abs(yp - y) <= acc_range:\n",
    "                        acc_count = acc_count + 1\n",
    "\n",
    "                    test_total = test_total + 1\n",
    "\n",
    "                    print(f\"\\t\\t accuracy={acc_count / test_total}\")\n",
    "\n",
    "class SaveModelCallback(Callback):\n",
    "    def __init__(self, filepath,  n=100, use_model_name=False):\n",
    "        super(SaveModelCallback, self).__init__()\n",
    "        self.num_epochs = 100\n",
    "        self.filepath = filepath\n",
    "        if use_model_name:\n",
    "            self.filepath = self.filepath + \"/\" + self.model.name\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        super(SaveModelCallback, self).on_epoch_end(epoch, logs)\n",
    "        if epoch % 100 == 0:\n",
    "            self.model.save(f\"{self.filepath}/{epoch}.model\")\n",
    "            print(f\"model saved: {self.filepath}/{epoch}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nondense_model():\n",
    "    model_name = \"nondense_model\"\n",
    "    \n",
    "    if not os.path.exists(cb_filepath + \"/\" + model_name):\n",
    "        os.makedirs(cb_filepath + \"/\" + model_name)  \n",
    "    \n",
    "    checkpoints = [cb_filepath + '/' + model_name + \"/\" + name\n",
    "                   for name in os.listdir(cb_filepath + \"/\" + model_name)]\n",
    "    if checkpoints:\n",
    "        latest_cp = max(checkpoints, key=os.path.getctime )\n",
    "        print('Restoring from', latest_cp)\n",
    "        return load_model(latest_cp)\n",
    "    \n",
    "    # Input layer of 3 neurons \n",
    "    inp = Input(shape=(1,3))\n",
    "    \n",
    "    #128 layer\n",
    "    d2_out = Dense(128)(inp)\n",
    "\n",
    "    #grab first, 2nd half of the 128 layer\n",
    "    d2_out_p1 = Lambda(lambda x: x[:,:,0:64])(d2_out)\n",
    "    d2_out_p2 = Lambda(lambda x: x[:,:,64:128])(d2_out)\n",
    "\n",
    "    #64 layer(s)\n",
    "    d3_out = Dense(64)(d2_out_p1)\n",
    "    d4_out = Dense(64)(d2_out_p2)\n",
    "\n",
    "    #grab output nodes from both 64 layers\n",
    "    d5_out = concatenate([d3_out, d4_out])\n",
    "    \n",
    "    o = Dense(1)(d5_out)\n",
    "    \n",
    "    model = Model(inp, o)\n",
    "    \n",
    "    model._name = model_name\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"MeanSquaredError\",\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dense_model_5L():\n",
    "    model_name = \"dense_model_5L\"\n",
    "    \n",
    "    if not os.path.exists(cb_filepath + \"/\" + model_name):\n",
    "        os.makedirs(cb_filepath + \"/\" + model_name)  \n",
    "    \n",
    "    checkpoints = [cb_filepath + '/' + model_name + \"/\" + name\n",
    "                   for name in os.listdir(cb_filepath + \"/\" + model_name)]\n",
    "    if checkpoints:\n",
    "        latest_cp = max(checkpoints, key=os.path.getctime )\n",
    "        print('Restoring from', latest_cp)\n",
    "        return load_model(latest_cp)\n",
    "    \n",
    "    model_5layer = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(1024, input_shape=(1,3)),\n",
    "        tf.keras.layers.Dense(512),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "#     model.name = \"dense_model\"\n",
    "\n",
    "    model_5layer._name = model_name\n",
    "    model_5layer.compile(\n",
    "        loss=\"MeanSquaredError\",\n",
    "        metrics=['accuracy'] #Acc not working, in testing\n",
    "    )\n",
    "\n",
    "    return model_5layer\n",
    "\n",
    "def dense_model2():\n",
    "    model_name = \"dense_model2\"\n",
    "    \n",
    "    if not os.path.exists(cb_filepath + \"/\" + model_name):\n",
    "        os.makedirs(cb_filepath + \"/\" + model_name)  \n",
    "    \n",
    "    checkpoints = [cb_filepath + '/' + model_name + \"/\" + name\n",
    "                   for name in os.listdir(cb_filepath + \"/\" + model_name)]\n",
    "    if checkpoints:\n",
    "        latest_cp = max(checkpoints, key=os.path.getctime )\n",
    "        print('Restoring from', latest_cp)\n",
    "        return load_model(latest_cp)\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(2048, input_shape=(1,3)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1024))\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Dense(64))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "#     model.name = \"dense_model v2\"\n",
    "    model._name = model_name\n",
    "    model.compile(\n",
    "        loss=\"MeanSquaredError\",\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Input(shape=(1,2)))\n",
    "#     model.add(Dense(64))\n",
    "#     model.add(Dense(1))\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=\"MeanSquaredError\",\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_examples = 20000\n",
    "rstart = 1\n",
    "rend = 10000\n",
    "\n",
    "# MULT\n",
    "mult_setX, mult_setY = ad.gen_data_mult(num_examples, rstart, rend)\n",
    "mult_setX = mult_setX.reshape(num_examples, 1, 3)\n",
    "\n",
    "# LOG(MULT)\n",
    "mult_logX = np.log(mult_setX)\n",
    "mult_logY = np.log(mult_setY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build ModelComparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelComparator()\n",
    "\n",
    "mc.add_model(\"nondense_model\", nondense_model)\n",
    "mc.add_model(\"dense_model\", dense_model_5L)\n",
    "mc.add_model(\"dense_model v2\", dense_model2)\n",
    "\n",
    "mc.add_training_set(\"log normal multiply\", (mult_logX, mult_logY))\n",
    "mc.add_training_set(\"multiplication\", (mult_setX, mult_setY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data (Garbage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.ones((100,2))\n",
    "# y = np.ones((100,1))\n",
    "\n",
    "# model.fit(X, y, epochs=25, batch_size=1, callbacks=[smc])\n",
    "# model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"checkpoints/chkpt-20.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL nondense_model TRAINING ON DATASET log normal multiply\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dd4b5a986f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m mc.train_models(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-52a5dd93cc8f>\u001b[0m in \u001b[0;36mtrain_models\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{start_dir}/{tset_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{start_dir}/{tset_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = num_examples\n",
    "\n",
    "mc.train_models(\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[smc]\n",
    ")\n",
    "\n",
    "# new_model = dense_model_5L()\n",
    "# new_model = load_model(\"checkpoints/chkpt-200.model\")\n",
    "\n",
    "# new_model.predict(mult_logX)\n",
    "#new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare min loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_min_loss, min_loss = mc.compare_min(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
